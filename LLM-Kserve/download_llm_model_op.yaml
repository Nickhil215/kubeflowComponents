name: Download LLM Model
description: Downloads a HuggingFace model and tokenizer to a specified local path.

inputs:
  - {name: model_id, type: String, description: HuggingFace model ID}
  - {name: hf_token, type: String, description: HuggingFace access token}

outputs:
  - {name: model_path, type: String, description: Path where the model is saved locally}

implementation:
  container:
    image: python:3.10
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet transformers==4.38.2 peft==0.8.2 hf_xet
        "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import argparse
        from transformers import AutoTokenizer, AutoModelForCausalLM

        def _make_parent_dirs_and_return_path(file_path: str):
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def download_llm_model(model_id, hf_token, model_path):
            print(f"Downloading model from HF: {model_id}")
            model = AutoModelForCausalLM.from_pretrained(model_id, token=hf_token)
            tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)

            print(f"Saving model to {model_path}")
            model.save_pretrained(model_path)
            tokenizer.save_pretrained(model_path)

            return model_path

        import argparse
        _parser = argparse.ArgumentParser()
        _parser.add_argument("--model_id", type=str, required=True)
        _parser.add_argument("--hf_token", type=str, required=True)
        _parser.add_argument("--model_path", type=_make_parent_dirs_and_return_path, required=True)
        _parsed_args = vars(_parser.parse_args())

        _result = download_llm_model(**_parsed_args)

        # Write output to file
        with open(_parsed_args["model_path"] + ".txt", "w") as f:
            f.write(str(_result))
    args:
      - --model_id
      - {inputValue: model_id}
      - --hf_token
      - {inputValue: hf_token}
      - --model_path
      - {outputPath: model_path}
