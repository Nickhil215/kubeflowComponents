name: Download LLM Model
description: Downloads a HuggingFace model and tokenizer to a specified local path and optionally uploads to S3.

inputs:
  - {name: model_id, type: string, description: HuggingFace model ID}
  - {name: hf_token, type: string, description: HuggingFace access token}
  - {name: upload_to_s3, type: boolean, default: false, optional: true, description: Whether to upload the model to S3}
  - {name: s3_bucket, type: string, optional: true, description: S3 bucket name}
  - {name: s3_key, type: string, optional: true, description: S3 object key}

outputs:
  - {name: model_path, type: string, description: Path where the model is saved locally}

implementation:
  container:
    image: mohitverma1688/base_image:v0.1
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet requests || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet requests --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import sys
        import tarfile
        import argparse
        import boto3
        from transformers import AutoTokenizer, AutoModelForCausalLM

        parser = argparse.ArgumentParser()
        parser.add_argument("--model_id", type=str, required=True)
        parser.add_argument("--hf_token", type=str, required=True)
        parser.add_argument("--upload_to_s3", type=bool, default=False)
        parser.add_argument("--s3_bucket", type=str, default=None)
        parser.add_argument("--s3_key", type=str, default=None)
        parser.add_argument("--model_path", type=str, required=True)
        args = parser.parse_args()

        os.makedirs(args.model_path, exist_ok=True)

        print(f"Downloading model from HF: {args.model_id}")
        model = AutoModelForCausalLM.from_pretrained(args.model_id, token=args.hf_token)
        tokenizer = AutoTokenizer.from_pretrained(args.model_id, token=args.hf_token)

        print(f"Saving model to {args.model_path}")
        model.save_pretrained(args.model_path)
        tokenizer.save_pretrained(args.model_path)

        if args.upload_to_s3 and args.s3_bucket and args.s3_key:
            print("Uploading to S3...")
            tar_path = "/tmp/model.tar.gz"
            with tarfile.open(tar_path, "w:gz") as tar:
                tar.add(args.model_path, arcname=os.path.basename(args.model_path))
            s3 = boto3.client("s3")
            s3.upload_file(tar_path, args.s3_bucket, args.s3_key)
            print(f"Uploaded to s3://{args.s3_bucket}/{args.s3_key}")
        else:
            print("S3 upload skipped.")
    args:
      - --model_id
      - {inputValue: model_id}
      - --hf_token
      - {inputValue: hf_token}
      - --upload_to_s3
      - {inputValue: upload_to_s3}
      - --s3_bucket
      - {inputValue: s3_bucket}
      - --s3_key
      - {inputValue: s3_key}
      - --model_path
      - {outputPath: model_path}
