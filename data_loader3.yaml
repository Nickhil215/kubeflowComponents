name: Data Loader Component
description: |-
  Data loader component that supports multiple input formats and preprocessing options.
  
      Annotations:
          author: Your Name <your.email@example.com>
inputs:
- {name: data_source, type: String, description: "Path or URL to input file (supports CSV, JSON, Excel, SQLite, ZIP, YAML)"}
- {name: val_size, type: Float, default: "0.2", description: "Validation set size (between 0 and 1)", optional: true}
- {name: stratify_column, type: String, description: "Column to use for stratified split", optional: true}
- {name: preprocessing_steps, type: JsonObject, description: "JSON object defining preprocessing steps", optional: true}
- {name: random_seed, type: Integer, default: "42", description: "Random seed for reproducibility", optional: true}
outputs:
- {name: train_data, type: CSV, description: "Training dataset output"}
- {name: validation_data, type: CSV, description: "Validation dataset output"}
- {name: metadata, type: JsonObject, description: "Metadata about the split and preprocessing"}
metadata:
  annotations:
    author: Your Name <your.email@example.com>
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - |
      python3 -m pip install --quiet --no-warn-script-location \
        'pandas==2.2.2' \
        'numpy==1.24.1' \
        'scikit-learn==1.3.0' \
        'pyyaml==6.0.1' \
        'requests==2.31.0' \
      && "$0" "$@"
    - python3
    - -u
    - -c
    - |
      import pandas as pd
      import numpy as np
      import json
      import logging
      import argparse
      from pathlib import Path
      import requests
      import yaml
      import sqlite3
      import zipfile
      from sklearn.model_selection import train_test_split
      import os
      from typing import Tuple, Dict, Union, List

      def serialize_json(data):
          """Serialize data to JSON string"""
          return json.dumps(data)

      def serialize_dataframe(df):
          """Serialize DataFrame to CSV string"""
          return df.to_csv(index=False)

      class DataLoader:
          def __init__(self):
              self.logger = self._setup_logger()
          
          @staticmethod
          def _setup_logger():
              logging.basicConfig(
                  level=logging.INFO,
                  format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
              )
              return logging.getLogger('DataLoader')

          def _download_from_url(self, url: str, temp_path: str) -> str:
              """Download data from URL"""
              try:
                  self.logger.info("Downloading data from: %s" % url)
                  response = requests.get(url, stream=True)
                  response.raise_for_status()
                  
                  with open(temp_path, 'wb') as f:
                      for chunk in response.iter_content(chunk_size=8192):
                          f.write(chunk)
                  
                  return temp_path
                  
              except Exception as e:
                  self.logger.error("Error downloading from URL: %s" % str(e))
                  raise

          def _load_data(self, data_source: str) -> pd.DataFrame:
              """Load data from various sources into DataFrame"""
              try:
                  # Handle URL
                  if data_source.startswith(('http://', 'https://')):
                      temp_path = 'temp_download'
                      data_source = self._download_from_url(data_source, temp_path)
                      
                  file_ext = Path(data_source).suffix.lower()
                  
                  # Load based on file extension
                  if file_ext == '.csv':
                      df = pd.read_csv(data_source)
                  elif file_ext == '.json':
                      df = pd.read_json(data_source)
                  elif file_ext in ['.xlsx', '.xls']:
                      df = pd.read_excel(data_source)
                  elif file_ext == '.db':
                      conn = sqlite3.connect(data_source)
                      table_name = data_source.split(':')[-1]
                      df = pd.read_sql(f"SELECT * FROM {table_name}", conn)
                      conn.close()
                  elif file_ext == '.zip':
                      with zipfile.ZipFile(data_source, 'r') as zip_ref:
                          first_file = zip_ref.namelist()[0]
                          with zip_ref.open(first_file) as f:
                              if first_file.endswith('.csv'):
                                  df = pd.read_csv(f)
                              elif first_file.endswith('.json'):
                                  df = pd.read_json(f)
                              else:
                                  raise ValueError("Unsupported file format in zip: %s" % first_file)
                  elif file_ext == '.yaml' or file_ext == '.yml':
                      with open(data_source, 'r') as f:
                          data = yaml.safe_load(f)
                      df = pd.DataFrame(data)
                  else:
                      raise ValueError("Unsupported file format: %s" % file_ext)
                      
                  return df
                  
              except Exception as e:
                  self.logger.error("Error loading data: %s" % str(e))
                  raise
              finally:
                  if data_source == 'temp_download' and os.path.exists(data_source):
                      os.remove(data_source)

          def process_data(self, data_source: str, val_size: float = 0.2,
                         stratify_column: str = None, preprocessing_steps: List[Dict] = None,
                         random_seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:
              """Process and split data"""
              try:
                  # Load data
                  df = self._load_data(data_source)
                  self.logger.info("Loaded %d samples" % len(df))
                  
                  # Apply preprocessing steps if provided
                  if preprocessing_steps:
                      df = self._apply_preprocessing(df, preprocessing_steps)
                  
                  # Validate val_size
                  val_size = float(val_size)
                  if not 0 < val_size < 1:
                      raise ValueError("Invalid validation size: %s" % val_size)
                  
                  # Prepare stratification if requested
                  stratify = df[stratify_column] if stratify_column else None
                  
                  # Split data
                  train_df, val_df = train_test_split(
                      df,
                      test_size=val_size,
                      random_state=random_seed,
                      stratify=stratify
                  )
                  
                  # Prepare metadata
                  metadata = {
                      'train_samples': len(train_df),
                      'val_samples': len(val_df),
                      'features': list(df.columns),
                      'data_shape': list(df.shape),
                      'column_types': dict((str(k), str(v)) for k, v in df.dtypes.items()),
                      'data_source': data_source,
                      'preprocessing_applied': preprocessing_steps if preprocessing_steps else [],
                      'stratified_split': bool(stratify_column),
                      'stratify_column': stratify_column if stratify_column else None,
                      'random_seed': random_seed,
                      'basic_stats': {
                          'numerical_columns': df.select_dtypes(include=[np.number]).columns.tolist(),
                          'categorical_columns': df.select_dtypes(include=['object']).columns.tolist(),
                          'missing_values': df.isnull().sum().to_dict(),
                          'unique_values': {col: df[col].nunique() for col in df.columns}
                      }
                  }
                  
                  self.logger.info("Split into %d training and %d validation samples" % 
                                (len(train_df), len(val_df)))
                  
                  return train_df, val_df, metadata
                  
              except Exception as e:
                  self.logger.error("Error processing data: %s" % str(e))
                  raise

          def _apply_preprocessing(self, df: pd.DataFrame, steps: List[Dict]) -> pd.DataFrame:
              """Apply preprocessing steps to DataFrame"""
              for step in steps:
                  step_type = step['type']
                  params = step.get('params', {})
                  
                  if step_type == 'drop_columns':
                      df = df.drop(columns=params['columns'])
                  elif step_type == 'fill_na':
                      df = df.fillna(params['value'])
                  elif step_type == 'encode_categorical':
                      for col in params['columns']:
                          df[col] = pd.Categorical(df[col]).codes
                  elif step_type == 'normalize':
                      for col in params['columns']:
                          df[col] = (df[col] - df[col].mean()) / df[col].std()
                  
              return df

      def _make_parent_dirs_and_return_path(file_path: str):
          import os
          os.makedirs(os.path.dirname(file_path), exist_ok=True)
          return file_path

      parser = argparse.ArgumentParser(description='Data Loader Component')
      parser.add_argument('--data-source', type=str, required=True,
                        help='Path to input file')
      parser.add_argument('--val-size', type=float, default=0.2,
                        help='Validation set size')
      parser.add_argument('--stratify-column', type=str,
                        help='Column to use for stratified split')
      parser.add_argument('--preprocessing-steps', type=json.loads,
                        help='JSON string defining preprocessing steps')
      parser.add_argument('--random-seed', type=int, default=42,
                        help='Random seed for reproducibility')
      parser.add_argument('--train-data', dest='train_output_path', type=_make_parent_dirs_and_return_path,
                        required=True, help='Path for training data output')
      parser.add_argument('--validation-data', dest='val_output_path', type=_make_parent_dirs_and_return_path,
                        required=True, help='Path for validation data output')
      parser.add_argument('--metadata', dest='metadata_output_path', type=_make_parent_dirs_and_return_path,
                        required=True, help='Path for metadata output')
      
      args = parser.parse_args()
      
      try:
          loader = DataLoader()
          
          # Process data
          train_df, val_df, metadata = loader.process_data(
              data_source=args.data_source,
              val_size=args.val_size,
              stratify_column=args.stratify_column,
              preprocessing_steps=args.preprocessing_steps,
              random_seed=args.random_seed
          )
          
          # Save outputs
          train_df.to_csv(args.train_output_path, index=False)
          val_df.to_csv(args.val_output_path, index=False)
          with open(args.metadata_output_path, 'w') as f:
              json.dump(metadata, f, indent=2)
              
          print("Data processing completed successfully!")
              
      except Exception as e:
          logging.error("Error: %s" % str(e))
          exit(1)
    args:
    - --data-source
    - {inputPath: data_source}
    - if:
        cond: {isPresent: val_size}
        then:
        - --val-size
        - {inputValue: val_size}
    - if:
        cond: {isPresent: stratify_column}
        then:
        - --stratify-column
        - {inputValue: stratify_column}
    - if:
        cond: {isPresent: preprocessing_steps}
        then:
        - --preprocessing-steps
        - {inputValue: preprocessing_steps}
    - if:
        cond: {isPresent: random_seed}
        then:
        - --random-seed
        - {inputValue: random_seed}
    - --train-data
    - {outputPath: train_data}
    - --validation-data
    - {outputPath: validation_data}
    - --metadata
    - {outputPath: metadata}
